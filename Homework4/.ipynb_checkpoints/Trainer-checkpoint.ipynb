{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Wrjxt74b46is",
   "metadata": {
    "id": "Wrjxt74b46is"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "OGWc1HS95GKZ",
   "metadata": {
    "id": "OGWc1HS95GKZ"
   },
   "source": [
    "# Trainer\n",
    ">* You can follow your work in HW3 to complete this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73846386",
   "metadata": {
    "id": "73846386"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "import math\n",
    "\n",
    "import import_ipynb\n",
    "from Loss import *\n",
    "from Model import *\n",
    "from Utils import random_mini_batches\n",
    "from Utils import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cff696d",
   "metadata": {
    "id": "7cff696d"
   },
   "outputs": [],
   "source": [
    "def trainer(model, config, x_train_pca, y_train, package=None, debug=False):\n",
    "    losses = []\n",
    "    # Loop (gradient descent)\n",
    "    xtr, ytr, xvl, yvl = None, None, None, None\n",
    "    if debug:\n",
    "        xtr, ytr, xvl, yvl = package\n",
    "        \n",
    "    for i in range(0, config.num_iterations):\n",
    "        mini_batches = random_mini_batches(x_train_pca, y_train, config.batch_size)\n",
    "        loss = 0\n",
    "        for batch in mini_batches:\n",
    "            x_batch, y_batch = batch\n",
    "\n",
    "            ### START CODE (refer to HW3) ###\n",
    "\n",
    "            # forward\n",
    "            AL = model.forward(x_batch)\n",
    "\n",
    "            # compute loss\n",
    "            if config.loss_function == 'cross_entropy':\n",
    "                loss += compute_BCE_loss(AL, y_batch)\n",
    "            elif config.loss_function == 'focal_loss':\n",
    "                loss += compute_focal_loss(AL, y_batch, config.alpha, config.gamma)\n",
    "\n",
    "            # backward\n",
    "            dA_prev = model.backward(AL, y_batch)\n",
    "            # update\n",
    "            model.update(config.learning_rate)\n",
    "\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "        loss /= len(mini_batches)\n",
    "        losses.append(loss)\n",
    "        if config.print_loss and i % config.print_freq == 0:\n",
    "            print (\"Loss after iteration %i: %f\" %(i, loss))\n",
    "            if debug:\n",
    "                print(\"==== Training ====\")\n",
    "                pred_train = predict(xtr, ytr, model)\n",
    "                print(\"==== Validation ====\")\n",
    "                pred_train = predict(xvl, yvl, model)\n",
    "                print()\n",
    "\n",
    "\n",
    "    # plot the loss\n",
    "    plt.figure(figsize=(4, 2))\n",
    "    plt.plot(np.squeeze(losses))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(config.learning_rate))\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc07a8c6aa785ccbb5cb0815abffaffb139b111aca417c4ffe9bf221bae76ba2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
